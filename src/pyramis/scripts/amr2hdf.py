#!/usr/bin/env python3

import h5py
import numpy as np
import os
import time, datetime
from tqdm import tqdm
import argparse
from packaging.version import Version
from typing import Optional, Tuple

from pyramis.utils import Timestamp, hilbert3d_map
from pyramis.utils.arrayview import SharedView
from pyramis import io, get_dim_keys
import tomllib

io.config['VNAME_GROUP'] = 'native' # Recommended to use native variable names


def create_hdf5_part(path, iout, n_chunk:int, size_load:int, converted_dtypes, output_path:str='../hdf', cpu_list=None, dataset_kw:dict={}, overwrite:bool=True, sim_description:str='', sim_publication:str='', version:str='1.0', nthread=8):
    info = io.get_info(path, iout)

    if cpu_list is None:
        cpu_list = np.arange(1, info['ncpu'] + 1, dtype='i4')
    else:
        cpu_list = np.sort(cpu_list)

    output_dir = os.path.join(path, output_path)
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f'part_{iout:05d}.h5')
    if os.path.exists(output_file) and not overwrite:
        try:
            with h5py.File(output_file, 'r') as fl:
                if Version(fl.attrs.get('version', '0.0')) < Version(version):
                    print(f"File {output_file} exists but with lower version ({fl.attrs.get('version', '0.0')} < {version}). Overwriting.")
                else:
                    # If the file exists and the version matches, skip creation
                    timer.message(f"File {output_file} already exists with matching version. Skipping creation.")
                    return
        except (KeyError, OSError) as e:
            print(f"File {output_file} exists but is not a valid HDF5 file. Overwriting.")
    
    timer.message(f"Generating new part dictionary for iout = {iout} with {len(cpu_list)} CPUs...")
    new_part_dict, pointer_dict = get_new_part_dict(path, iout, cpu_list=cpu_list, size_load=size_load, converted_dtypes=converted_dtypes, nthread=nthread)
    names = new_part_dict.keys()
    
    timer.message(f"Creating HDF5 file {output_file} with {len(new_part_dict)} particle types...")
    with h5py.File(output_file, 'w') as fl:
        fl.attrs['publication'] = sim_publication
        fl.attrs['description'] = 'Ramses particle data' \
        "\n============================================================================" \
        "\nThis file contains particle data extracted from Ramses snapshots in HDF5 format. "\
        "It includes particle coordinates, velocities, masses, and other properties. "\
        "The data is organized in chunks based on Hilbert keys for efficient access. "\
        "The data within each chunk is sorted by level. "\
        f"\nThis file is generated by {__file__} script." \
        "\nThis file contains following particle types:"\
        "\n'star': stellar particles"\
        "\n'dm': dark matter particles"\
        "\n'cloud': cloud particles, temporal particles generated for sink particles."\
        "\n'tracer': tracer particles, includes gas, star, sink tracers."\
        "\n'sink': sink particles, represent MBH population."\
        "\nEach particle type has the following datasets:"\
        "\n'data': Particle data."\
        "\n'hilbert_boundary': Hilbert key boundaries for each chunk based on the Peano-Hilbery curve with levelmax resolution." \
        "\n'chunk_boundary': Chunk boundary indices for each chunk." \
        "\n'level_boundary': Level boundary indices for each level within chunks." \
        '\n' + sim_description
        fl.attrs['created'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        add_basic_attrs(fl, info)
        add_attr_with_descr(fl, 'cpulist', cpu_list, 'List of CPU indices used for this snapshot.')

        n_level = info['nlevelmax']
        add_attr_with_descr(fl, 'n_level', n_level, 'Number of levels in the snapshot.')
        add_attr_with_descr(fl, 'n_chunk', n_chunk, 'Number of chunks in the snapshot.')

        add_attr_with_descr(fl, 'script', os.path.basename(__file__), 'Name of the script used to generate the file.')

        n_part_tot = 0
        for name in names:
            new_part = new_part_dict[name][:pointer_dict[name]]
            if new_part.size == 0:
                print(f"No particles of type {name} found in iout = {info['iout']}. Skipping export.")
                continue

            # Add particle data to HDF5 file
            add_group(fl, name, new_part,
                      levelmin=info['levelmin'], levelmax=info['levelmax'],
                      n_chunk=n_chunk, n_level=n_level, part=True, dataset_kw=dataset_kw)

            n_part_tot += new_part.size
        
        add_attr_with_descr(fl, 'size', n_part_tot, 'Total number of particles in the snapshot.')
        add_attr_with_descr(fl, 'version', version, 'Version of the file.')

def get_new_part_dict(path:str, iout:int, cpu_list, size_load, converted_dtypes, nthread=8) -> Tuple[dict, dict]:
    """
    Get a new dictionary to store particle data for each type.
    """
    names = converted_dtypes.keys()
    info = io.get_info(path, iout)

    # pre-define the new particle array based on the snapshot header
    new_part_dict = {}
    pointer_dict = {}

    #header = snap.extract_header()    
    header = io.read_npart_header(path, iout)
    for name in names:
        if name == 'sink':
            try:
                part = io.read_sink(path=path, iout=iout)
                header[name] = part.size if part is not None else 0
            except ValueError:
                print(f"No sink data available for iout = {iout}. Skipping sink export.")
                header[name] = 0
        if header[name] == 0:
            continue
        new_dtypes = converted_dtypes[name]
        new_dtypes = [tuple(d) for d in new_dtypes]
        new_part_dict[name] = np.empty(header[name], dtype=new_dtypes)
        pointer_dict[name] = 0

    names = new_part_dict.keys() # update names to only include available data
    idx_array = np.arange(len(cpu_list))[::size_load]
    for idx in idx_array:
        timer.message(f"Loading particles ({idx} / {len(cpu_list)})...")
        cpu_list_sub = cpu_list[idx:np.minimum(idx + size_load, len(cpu_list))]
        if len(cpu_list_sub) == 0:
            continue

        part_data = io.read_part(path=path, iout=iout, cpulist=cpu_list_sub, read_cpu=True, n_workers=nthread, use_process=True, copy_result=False)

        if part_data is None:
            raise ValueError("Particle not loaded in snapshot")
        
        # sort the particle data in each hilbert domain to save sorting time later
        npart_per_cpu = io.read_npart_per_cpu(path, iout, cpu_list_sub)
        for offset, n in zip(np.cumsum(npart_per_cpu), npart_per_cpu):
            part_slice = part_data[offset - n:offset]
            hkey = get_hilbert_key(np.asarray([part_slice[key] for key in get_dim_keys()]).T, info['nlevelmax'], nthread=nthread)
            sort_idx = np.argsort(hkey, kind='mergesort')
            part_data[offset-n:offset] = part_slice[sort_idx]
        
        for name in names:
            new_part = new_part_dict[name]
            new_dtypes = converted_dtypes[name]
            if name == 'sink':
                if pointer_dict[name] == 0: # we load sink data only once
                    part = io.read_sink(path=path, iout=iout)
                    hilbert_key = get_hilbert_key(np.asarray([part[key] for key in get_dim_keys()]).T, info['nlevelmax'], nthread=nthread)
                    part = part[np.argsort(hilbert_key)] # already Particle class at this point
                else: # sink data is alrady loaded
                    continue
            else:
                part = part_data[io.mask_by_part_type(part_data, name)]
                #part = uri.Particle(part_data, snap)[name]

            timer.message(f"Exporting {name} data with {part.size} particles..."
                  f"\nReduced item size: {part.dtype.itemsize} -> {new_part.dtype.itemsize} B ({new_part.dtype.itemsize / part.dtype.itemsize * 100:.2f}%)")

            for field in new_dtypes:
                new_part_dict[name][pointer_dict[name]:pointer_dict[name] + part.size][field[0]] = part[field[0]]
            pointer_dict[name] += part.size
        
        if isinstance(part_data, SharedView):
            part_data.close()
        del part_data
    
    # ensure the number of particles match the header
    for name in names:
        if cpu_list_sub.size == info['ncpu'] or name =='sink':
            assert pointer_dict[name] == header[name], f"Number mismatch for {name}: {pointer_dict[name]} != {header[name]}"
    return new_part_dict, pointer_dict

def compute_key_boundaries(key_array: np.ndarray, n_key: int) -> np.ndarray:
    """
    Compute the boundaries for each key based on the key array.
    key must be bewteen 0 and n_key-1.
    """
    key_boundaries = np.searchsorted(key_array, np.arange(n_key), side='right')
    key_boundaries = np.append(0, key_boundaries)
    return key_boundaries


def create_hdf5_cell(path, iout, n_chunk:int, size_load:int, converted_dtypes, output_path:str='../hdf', cpu_list=None, dataset_kw:dict={}, overwrite:bool=True, sim_description:str='', sim_publication:str='', version:str='1.0', nthread=8):
    """
    Export cell data from the snapshot to HDF5 format.
    """
    info = io.get_info(path, iout)
    if cpu_list is None:
        cpu_list = np.arange(1, info['ncpu'] + 1, dtype='i4')
    else:
        cpu_list = np.array(cpu_list, dtype='i4')

    output_dir = os.path.join(path, output_path)
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f'cell_{iout:05d}.h5')
    if os.path.exists(output_file) and not overwrite:
        try:
            with h5py.File(output_file, 'r') as fl:
                if Version(fl.attrs.get('version', '0.0')) < Version(version):
                    print(f"File {output_file} exists but with lower version ({fl.attrs['version']} < {version}). Overwriting.")
                else:
                    # If the file exists and the version matches, skip creation
                    timer.message(f"File {output_file} already exists with matching version. Skipping creation.")
                    return
        except (OSError, KeyError) as e:
            print(f"File {output_file} exists but is not a valid HDF5 file. Overwriting.")
    timer.message(f"Creating HDF5 file {output_file} for cells...")
    with h5py.File(output_file, 'w') as fl:
        fl.attrs['publication'] = sim_publication
        fl.attrs['description'] = 'Ramses cell/AMR data' \
        "\n============================================================================" \
        "\nThis file contains cell data extracted from Ramses snapshots in HDF5 format. " \
        "It includes cell coordinates, velocities, densities, and other properties. " \
        "The data is organized in chunks based on Hilbert keys for efficient access. " \
        "The data within each chunk is sorted by level. Please check 'attributes' for more information." \
        f"\nThis file is generated by {__file__} script." \
        "\nThis file contains following cell types:" \
        "\n'leaf': leaf cells with no children, can be used for general purpose." \
        "\n'branch': branch cells with children, can be used for quick access to the averaged quantities." \
        "\nEach cell type has following datasets:" \
        "\n'data': Cell data." \
        "\n'hilbert_boundary': Hilbert key boundaries for each chunk based on the Peano-Hilbery curve with levelmax resolution." \
        "\n'chunk_boundary': Chunk boundary indices for each chunk." \
        "\n'level_boundary': Level boundary indices for each level within chunks." \
        "\n============================================================================" \
        "\n" + sim_description
        fl.attrs['created'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        add_basic_attrs(fl, info)
        add_attr_with_descr(fl, 'cpulist', cpu_list, 'List of CPU indices used for this snapshot.')

        n_level = info['nlevelmax']
        add_attr_with_descr(fl, 'n_level', n_level, 'Number of levels in the snapshot.')
        add_attr_with_descr(fl, 'n_chunk', n_chunk, 'Number of chunks in the snapshot.')

        n_cell_tot = 0

        read_branch = None
        for name in ['leaf', 'branch']:
            timer.message(f"Generating new {name} cell array for iout = {iout} with {len(cpu_list)} CPUs...")
            if name == 'leaf':
                read_branch = False
            elif name == 'branch':
                read_branch = True
            else:
                raise ValueError(f"Unknown cell type: {name}")
            new_cell, pointer = get_new_cell(path, iout, cpu_list=cpu_list, size_load=size_load, converted_dtypes=converted_dtypes, read_branch=read_branch, nthread=nthread)
            new_cell = new_cell[:pointer]

            # Add cell data to HDF5 file
            add_group(fl, name, new_cell,
                      levelmin=info['levelmin'], levelmax=info['levelmax'],
                      n_chunk=n_chunk, n_level=n_level, part=False, dataset_kw=dataset_kw)

            n_cell_tot += new_cell.size

        add_attr_with_descr(fl, 'size', n_cell_tot, 'Total number of cells in the snapshot.')
        add_attr_with_descr(fl, 'version', version, 'Version of the file.')


def get_new_cell(path, iout, cpu_list, size_load, converted_dtypes, read_branch=False, nthread=8) -> Tuple[np.ndarray, int]:
    """
    Get a new array to store cell data.
    """

    info = io.get_info(path, iout)

    new_cell = None
    pointer = 0

    timer.message(f"Calculating total number of cells for iout = {iout} with {len(cpu_list)} CPUs...")
    n_cell = np.sum(io.read_ncell_per_cpu(path, iout, cpulist=cpu_list, read_branch=read_branch))

    new_dtypes = [tuple(d) for d in converted_dtypes]
    new_cell = np.empty(n_cell, dtype=new_dtypes)

    # sub-load cell data
    idx_array = np.arange(len(cpu_list))[::size_load]
    for idx in idx_array:
        timer.message(f"Loading cells ({idx} / {len(cpu_list)})...")
        cpu_list_sub = cpu_list[idx:np.minimum(idx + size_load, len(cpu_list))]
        if len(cpu_list_sub) == 0:
            continue
        cell_data = io.read_cell(path=path, iout=iout, cpulist=cpu_list_sub, read_branch=read_branch, read_hydro=True, read_grav=True, read_cpu=True, n_workers=nthread, use_process=True, copy_result=False)

        # sort the cell data in each hilbert domain to save sorting time later
        ncell_per_cpu = io.read_ncell_per_cpu(path, iout, cpu_list_sub, read_branch=read_branch)
        for offset, n in zip(np.cumsum(ncell_per_cpu), ncell_per_cpu):
            cell_slice = cell_data[offset - n:offset]
            hkey = get_hilbert_key(np.asarray([cell_slice[key] for key in get_dim_keys()]).T, info['nlevelmax'], nthread=nthread)
            sort_idx = np.argsort(hkey, kind='mergesort')
            cell_data[offset-n:offset] = cell_slice[sort_idx]

        timer.message(f"Exporting cell data with {cell_data.size} cells..."
              f"\nItem size: {cell_data.dtype.itemsize} -> {new_cell.dtype.itemsize} B ({new_cell.dtype.itemsize / cell_data.dtype.itemsize * 100:.2f}%)")
        for field in new_dtypes:
            new_cell[pointer:pointer + cell_data.size][field[0]] = cell_data[field[0]]
        pointer += cell_data.size
        if isinstance(cell_data, SharedView):
            cell_data.close()
        del cell_data
    return new_cell, pointer


def export_snapshots(path, iout_list, n_chunk, size_load, converted_dtypes_part=None, converted_dtypes_cell=None, output_path:str='../hdf', cpu_list=None, dataset_kw:dict={}, overwrite:bool=True, sim_description:str='', sim_publication:str='', version:str='1.0', nthread:int=8, walltime=None, convert_part=True, convert_cell=True):
    """
    Export snapshots from the repository to HDF5 format.
    This function will export both particle and cell data.
    """

    vname_abbr = io.config['VNAME_MAPPING'][io.config['VNAME_GROUP']]
    iout_avail = io.get_available_snapshots(path, check_data=['amr', 'hydro', 'part', 'grav'], scheduled_only=False)['iout']
    if iout_list is None:
        iout_list = iout_avail
    else:
        iout_list = iout_list[np.isin(iout_list, iout_avail)]

    info = io.get_info(path, iout_list[0])
    if size_load <= 0:
        size_load = info['ncpu']

    for iout in tqdm(iout_list, desc=f"Exporting snapshot data", disable=True):
        
        # remove unnecessary fields per perticle type
        if converted_dtypes_part is None and convert_part:
            converted_dtypes_part = {}
            dtype_part = io.read_part(path, iout, cpulist=[1], read_cpu=True).dtype
            for name in ['star', 'dm', 'cloud', 'tracer']:
                new_dtype = []
                for key, fmt in dtype_part.descr:
                    if name == 'dm' and key in [vname_abbr[key] if key in vname_abbr else key for key in ['metallicity', 'birth_time', 'initial_mass', 'birth_density']]:
                        continue
                    if name == 'cloud' and key in [vname_abbr[key] if key in vname_abbr else key for key in ['metallicity', 'birth_time', 'initial_mass', 'birth_density']]:
                        continue
                    if name == 'tracer' and key not in [vname_abbr[key] if key in vname_abbr else key for key in ['velocity_x', 'velocity_y', 'velocity_z', 'metallicity', 'birth_time', 'initial_mass', 'birth_density']]:
                        continue

                    dt = np.dtype(fmt)
                    if dt.kind == 'f' and dt.itemsize == 8 and key not in get_dim_keys():
                        new_fmt = np.dtype(dt.byteorder + 'f4')
                    else:
                        new_fmt = dt
                    
                    if key == 'level':
                        new_fmt = np.dtype(dt.byteorder + 'i1')
                    if key == 'cpu':
                        new_fmt = np.dtype(dt.byteorder + 'i2')

                    new_dtype.append((key, new_fmt))
                converted_dtypes_part[name] = new_dtype
            
            if os.path.exists(io.config['FILENAME_FORMAT'].format(data='sink', iout=iout, icpu=1)):
                dtype_sink = io.read_sink(path, iout).dtype
                new_dtype = []
                for desc in dtype_sink.descr:
                    key = desc[0]
                    fmt = desc[1]
                    dt = np.dtype(fmt)
                    if dt.kind == 'f' and dt.itemsize == 8 and key not in get_dim_keys():
                        new_fmt = np.dtype(dt.byteorder + 'f4')
                    else:
                        new_fmt = dt
                    new_dtype.append((key, new_fmt))
                converted_dtypes_part['sink'] = new_dtype
            
        if converted_dtypes_cell is None and convert_cell:
            converted_dtypes_cell = {}
            dtype_cell = io.read_cell(path, iout, cpulist=[1], read_hydro=True, read_grav=True, read_cpu=True).dtype
            for name in ['cell']:
                new_dtype = []
                for key, fmt in dtype_cell.descr:
                    dt = np.dtype(fmt)
                    if dt.kind == 'f' and dt.itemsize == 8 and key not in get_dim_keys():
                        new_fmt = np.dtype(dt.byteorder + 'f4')
                    else:
                        new_fmt = dt
                    
                    if key == 'level':
                        new_fmt = np.dtype(dt.byteorder + 'i1')
                    
                    if key == 'cpu':
                        new_fmt = np.dtype(dt.byteorder + 'i2')

                    new_dtype.append((key, new_fmt))

                converted_dtypes_cell[name] = new_dtype

        if iout not in iout_avail:
            timer.message(f"Snapshot iout = {iout} not available or missing data. Skipping.")
            continue
        # Start exporting cell and particle data for each snapshot
        if convert_part and converted_dtypes_part is not None:
            timer.start(f"Starting particle data extraction for iout = {iout}.", name='part_hdf')
            create_hdf5_part(path, iout, n_chunk=n_chunk, size_load=size_load, converted_dtypes=converted_dtypes_part, output_path=output_path, cpu_list=cpu_list, dataset_kw=dataset_kw, overwrite=overwrite, sim_description=sim_description, sim_publication=sim_publication, version=version, nthread=nthread)
            timer.record(f"Particle data extraction completed for iout = {iout}.", name='part_hdf')
        
        if convert_cell and converted_dtypes_cell is not None:
            timer.start(f"Starting cell data extraction for iout = {iout}.", name='cell_hdf')
            create_hdf5_cell(path, iout, n_chunk=n_chunk, size_load=size_load, converted_dtypes=converted_dtypes_cell['cell'], output_path=output_path, cpu_list=cpu_list, dataset_kw=dataset_kw, overwrite=overwrite, sim_description=sim_description, sim_publication=sim_publication, version=version, nthread=nthread)
            timer.record(f"Cell data extraction completed for iout = {iout}.", name='cell_hdf')

        if walltime is not None:
            elapsed = timer.time()
            if elapsed > walltime * 3600:
                timer.message(f"Walltime exceeded for iout = {iout}. Elapsed time: {elapsed / 3600:.2f} hours.")
                break

def get_hilbert_key(coordinates:np.ndarray, levelmax:int, levels=None, nthread:int=8) -> np.ndarray:
    # subdivisions = 2 ** levelmax
    #if levelmax > 21:
    #    raise ValueError("Levelmax must be less than or equal to 21 to avoid overflow in Hilbert key calculation.")
    # idx_list = np.floor(coordinates * subdivisions).astype(int)
    # npoints = idx_list.shape[0]
    if levels is None:
        return hilbert3d_map(coordinates, levelmax, kwargs_hilbert3d=dict(n_workers=nthread)).astype(np.float128)
    else:
        return hilbert3d_map(coordinates, levelmax, levels=levels, kwargs_hilbert3d=dict(n_workers=nthread)).astype(np.float128)

def get_chunk_boundaries(hilbert_key:np.ndarray, n_chunk:int) -> np.ndarray:
    """
    Get the boundary indices based on the given array of Hilbert key.
    The Hilbert key must be sorted in ascending order.
    The boundaries are determined by dividing the Hilbert key into `n_chunk` equal parts.
    Indices with same Hilbert key are grouped together.
    """
    chunk_boundary_exact = np.linspace(0, len(hilbert_key), n_chunk + 1).astype(int)
    # get hilbert keys for the exact chunk boundaries
    bound_key = hilbert_key[chunk_boundary_exact[1:-1]]
    lower_boundary = np.searchsorted(hilbert_key, bound_key, side='left')
    upper_boundary = np.searchsorted(hilbert_key, bound_key, side='right')
    # get either left or right boundaries that are closer to the index
    chunk_boundary = np.select([chunk_boundary_exact[1:-1] - lower_boundary < upper_boundary - chunk_boundary_exact[1:-1], True], [lower_boundary, upper_boundary])
    chunk_boundary = np.concatenate([[0], chunk_boundary, [len(hilbert_key)]])  # ensure the last boundary is the end of the array

    return chunk_boundary.astype(np.int32)


def assert_sorted(arr: np.ndarray):
    """
    Assert that the input array is sorted in ascending order.
    """
    try:
        assert np.all(arr[:-1] <= arr[1:])
    except AssertionError:
        idx = np.where(arr[:-1] > arr[1:])[0][0]
        print("At idx = ", idx, "value = ", arr[idx], arr[idx+1])
        raise AssertionError("Input array must be sorted in ascending order.")


def add_basic_attrs(fl: h5py.File, info: dict):
    """
    Add basic attributes to the HDF5 file.
    """

    add_attr_with_descr(fl, 'iout', info['iout'], 'Output index of the snapshot.')
    add_attr_with_descr(fl, 'icoarse', info['nstep_coarse'], 'Number of coarse time steps of the snapshot.')
    add_attr_with_descr(fl, 'ncpu', info['ncpu'], 'Number of CPUs used in the simulation.')
    add_attr_with_descr(fl, 'ndim', info['ndim'], 'Number of dimensions of the simulation.')

    add_attr_with_descr(fl, 'levelmin', info['levelmin'], 'Minimum level of the simulation.')
    add_attr_with_descr(fl, 'levelmax', info['levelmax'], 'Maximum level of the simulation.')
    add_attr_with_descr(fl, 'boxlen', info['boxlen'], 'Length of the simulation box.')

    add_attr_with_descr(fl, 'time', info['time'], 'Time of the snapshot.')
    add_attr_with_descr(fl, 'aexp', info['aexp'], 'Scale factor of the snapshot.')
    add_attr_with_descr(fl, 'age', info['age'], 'Age of the snapshot in Gyr.')
    add_attr_with_descr(fl, 'z', info['z'], 'Redshift of the snapshot.')

    add_attr_with_descr(fl, 'omega_m', info['omega_m'], 'Matter density parameter.')
    add_attr_with_descr(fl, 'omega_l', info['omega_l'], 'Dark energy density parameter.')
    add_attr_with_descr(fl, 'omega_k', info['omega_k'], 'Curvature density parameter.')
    add_attr_with_descr(fl, 'omega_b', info['omega_b'], 'Baryon density parameter.')
    add_attr_with_descr(fl, 'H0', info['H0'], 'Hubble constant at z=0.')

    add_attr_with_descr(fl, 'unit_l', info['unit_l'], 'Unit of length in cm.')
    add_attr_with_descr(fl, 'unit_d', info['unit_d'], 'Unit of density in g/cm^3.')
    add_attr_with_descr(fl, 'unit_t', info['unit_t'], 'Unit of time in s.')
    add_attr_with_descr(fl, 'unit_m', info['unit_d'] * info['unit_l']**3, 'Unit of mass in g.')
    add_attr_with_descr(fl, 'unit_v', info['unit_l'] / info['unit_t'], 'Unit of velocity in cm/s.')
    add_attr_with_descr(fl, 'unit_p', info['unit_d'] * info['unit_l']**2 / info['unit_t']**2, 'Unit of pressure in g/(cm*s^2).')

def add_attr_with_descr(fl: h5py.File, key: str, value, description: str):
    """
    Add an attribute to the HDF5 file with a description.
    """
    fl.attrs[key] = value
    if 'attributes' not in fl.attrs:
        fl.attrs['attributes'] = "This file includes the following attributes:"
    current_attrs = str(fl.attrs['attributes'])
    fl.attrs['attributes'] = current_attrs + f"\n'{key}': {description}"

def write_dataset(group:h5py.Group, dataset_name:str, data:np.ndarray, sort_key=None, mem_block_bytes=1000 * 1024**2, **dataset_kw):
    """
    Write a dataset to the HDF5 group with the specified name and data with allowing overwriting.
    """
    if dataset_name in group:
        del group[dataset_name]

    dset = group.create_dataset(dataset_name, shape=data.shape, dtype=data.dtype, **dataset_kw)
    if sort_key is not None:
        elems_per_row = int(np.prod(data.shape[1:], dtype=np.int64)) if data.ndim > 1 else 1
        itemsize_per_row = int(data.dtype.itemsize) * elems_per_row
        write_block = max(1, mem_block_bytes // itemsize_per_row)

        # create dataset with chunking to save memory
        n_total = data.shape[0]
        buf = np.empty((write_block,) + data.shape[1:], dtype=data.dtype)
        for start in range(0, n_total, write_block):
            end = min(start + write_block, n_total)
            k = end - start
            np.take(data, sort_key[start:end], out=buf[:k])
            dset[start:end] = buf[:k]
    else:
        # directly write the data without sorting
        dset[:] = data
    return dset

def add_group(fl:h5py.File, name:str, new_data:np.ndarray, levelmin:int, levelmax:int, n_chunk:int, n_level:int, part=False, sort=True, dataset_kw:dict={}):
    """
    Add a group to the HDF5 file with the specified name and data with chunks ordered by hilbert key and levels.
    """
    timer.message(f"Measuring Hilbert key for {name} data...")

    # compute chunk boundaries based on Hilbert key and sort the data accordingly
    coordinates = np.array([new_data[key] for key in get_dim_keys()]).T
    if not part:
        # use level information to compute Hilbert key for cells
        chunk_boundary, hilbert_boundary, sort_key1 = set_hilbert_boundaries(coordinates, new_data['level'], n_chunk, levelmax, part=part, sort=sort)
    else:
        # for particles, level information is not used
        chunk_boundary, hilbert_boundary, sort_key1 = set_hilbert_boundaries(coordinates, None, n_chunk, levelmax, part=part, sort=sort)

    level_boundary = None
    if new_data.dtype.names is None:
        raise ValueError("Data must be a structured array with named fields.")
    elif 'level' in new_data.dtype.names:
        # compute level boundaries within each chunk and sort the data accordingly
        levels = np.take(new_data['level'], sort_key1)
        level_boundary, sort_key2 = set_level_boundaries(levels, chunk_boundary, n_chunk, n_level)
        assert_sorted(level_boundary)
    
        # combine the two sort keys
        sort_key = sort_key1[sort_key2]
    else:
        sort_key = sort_key1

    if name in fl:
        del fl[name]
    grp = fl.create_group(name)
    grp.attrs['name'] = name
    grp.attrs['size'] = new_data.size

    grp.attrs['n_level'] = n_level
    grp.attrs['n_chunk'] = n_chunk
    grp.attrs['levelmax'] = levelmax
    grp.attrs['levelmin'] = levelmin

    write_dataset(grp, 'hilbert_boundary', data=hilbert_boundary, **dataset_kw)
    write_dataset(grp, 'chunk_boundary', data=chunk_boundary, **dataset_kw)
    if level_boundary is not None:
        write_dataset(grp, 'level_boundary', data=level_boundary, **dataset_kw)
    timer.message(f"Writing {name} data with {new_data.size} components...")
    write_dataset(grp, 'data', data=new_data, sort_key=sort_key, mem_block_bytes=1000 * 1024**2, **dataset_kw)

    return grp

def set_hilbert_boundaries(coordinates: np.ndarray, levels: Optional[np.ndarray], n_chunk: int, levelmax:int, part: bool=False, sort=True):
    """
    Sort the given data according to the Hilbert key, and returns the indices and Hilbert keys at chunk boundaries.
    If levels is provided, it will be used to compute the Hilbert key with level information.
    3D coordinates are expected to be in the range [0, 1).    
    """
    if part:
        hilbert_key = get_hilbert_key(coordinates, levelmax, nthread=8)
    else:
        hilbert_key = get_hilbert_key(coordinates, levelmax, levels, nthread=8)
    if coordinates.shape[0] > 1E8:
        timer.message(f"Sorting data with {coordinates.shape[0]} components...")
    if sort:
        sort_key = np.argsort(hilbert_key, kind='mergesort')
    else:
        sort_key = np.arange(hilbert_key.size)
    hilbert_key = hilbert_key[sort_key]
    # buf = np.empty_like(new_data)
    # np.take(new_data, sort_key, out=buf)
    # new_data[:] = buf
    assert_sorted(hilbert_key)

    timer.message("Getting chunk boundaries...")
    chunk_boundary = get_chunk_boundaries(hilbert_key, n_chunk)
    assert_sorted(chunk_boundary)

    hilbert_key_max = np.exp2(3 * levelmax, dtype=hilbert_key.dtype)  # maximum hilbert key value for levelmax

    # generate hilbert key for each chunk boundary
    safe_mask = chunk_boundary < hilbert_key.size
    hilbert_boundary = np.empty(n_chunk+1, dtype=hilbert_key.dtype)
    hilbert_boundary[safe_mask] = hilbert_key[chunk_boundary[safe_mask]]
    hilbert_boundary[0] = 0
    hilbert_boundary[~safe_mask] = hilbert_key_max
    assert_sorted(hilbert_boundary)

    return chunk_boundary, hilbert_boundary, sort_key

def set_level_boundaries(levels: np.ndarray, chunk_boundary: np.ndarray, n_chunk: int, n_level: int):
    """
    Get the level boundaries for the cell data after sorting by level within each chunk.
    """
    sort_key = np.arange(levels.size)
    for ichunk in range(n_chunk):
        bound = chunk_boundary[ichunk], chunk_boundary[ichunk + 1]
        if bound[0] == bound[1]:
            continue
        sl = slice(*bound)
        sort_key_local = np.argsort(levels[sl])
        sort_key[sl] = sort_key[sl][sort_key_local]
        levels[sl] = levels[sl][sort_key_local]
        # new_data[sl] = new_data[sl][sort_key]

    chunk_array = np.repeat(np.arange(n_chunk), chunk_boundary[1:] - chunk_boundary[:-1])
    key = chunk_array * n_level + (levels - 1)
    assert_sorted(key)
    level_boundary = compute_key_boundaries(key, n_key=n_chunk * n_level)
    return level_boundary, sort_key



def main(args):
    """
    Main function to extract data from the snapshot and save it to HDF5.
    """
    dataset_kw = args.dataset_kw if hasattr(args, 'dataset_kw') else {}

    iout_list = np.arange(args.imin, args.imax + 1)
    repo_path = args.repo
    overwrite = args.overwrite
    version = args.version

    # receive the snapshot from the simulation repository
    #snap = uri.RamsesSnapshot(repo_path, iout=iout_list[0], mode='nc')
    #repo = uri.RamsesRepo(snap)
    
    sim_publication = args.sim_publication if hasattr(args, 'sim_publication') else ""
    sim_description = args.sim_description if hasattr(args, 'sim_description') else ""

    converted_dtypes_part = args.converted_dtypes_part if hasattr(args, 'converted_dtypes_part') else None
    converted_dtypes_cell = args.converted_dtypes_cell if hasattr(args, 'converted_dtypes_cell') else None

    n_chunk = args.nchunk if hasattr(args, 'nchunk') else 1000

    size_load = args.nload
    nthread = args.nthread
    relative_output_path = args.output

    #iout_list = np.arange(0, snap.iout, 10)[::-1]
    #iout_list = [repo.get_snap(z=z).iout for z in [1.0, 1.25, 1.5, 1.75, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 8.0]][::-1]
    cpu_list = None
    export_snapshots(repo_path, iout_list=iout_list, n_chunk=n_chunk, size_load=size_load,
                     converted_dtypes_part=converted_dtypes_part, converted_dtypes_cell=converted_dtypes_cell,
                     output_path=relative_output_path, cpu_list=cpu_list, dataset_kw=dataset_kw,
                     sim_description=sim_description, sim_publication=sim_publication, version=version, overwrite=overwrite, nthread=nthread, walltime=args.walltime)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Convert Ramses snapshot data to HDF5 format.')
    print(f"Usage: {parser.prog} [options] <repo_path>")
    parser.add_argument("--repo", "-r", help='Repository path', type=str, default='.')
    parser.add_argument("--imin", "-i", help='Minimum output index to process (default: 1)', type=int, default=1)
    parser.add_argument("--imax", "-I", help='Maximum output index to process (default: 1)', type=int, default=1)
    parser.add_argument("--verbose", "-v", help='Enable verbose output', action='store_true')
    parser.add_argument("--version", "-vs", help='Version of the output files', type=str, default='1.1')
    parser.add_argument("--overwrite", "-o", help='Overwrite existing output files', action='store_true')
    parser.add_argument("--nload", "-l", help=f'Number of RAMSES files to load at once (default: -1)', type=int, default=-1)
    parser.add_argument("--nthread", "-t", help=f'Number of threads to use for loading data (default: 1)', type=int, default=1)
    parser.add_argument("--nchunk", "-n", help='Number of chunks to divide the data into (default: 100)', type=int, default=100)
    parser.add_argument("--walltime", "-w", help='Walltime limit (hours) for the job (default: None)', type=float, default=None)
    parser.add_argument("--config", "-c", help='Path to configuration file (default: None)', type=str, default=None)
    parser.add_argument("--output", "-p", help='Relative output path (default: hdf)', type=str, default='hdf')

    args = parser.parse_args()

    if args.config is not None:
        config_path = args.config

        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Configuration file {config_path} not found.")

        with open(config_path, "rb") as f:
            config = tomllib.load(f)

        for key, value in config.items():
            setattr(args, key, value)
    timer = Timestamp()
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print('-----------------------------------------------')
    print(f"RAMSES to HDF5: Script started at {now}")
    print(f"Input arguments: {args}")
    print('-----------------------------------------------')

    timer.start("Starting data export...", name='main')
    main(args)
    timer.record("Script completed successfully.", name='main')

    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print('-----------------------------------------------')
    print(f"RAMSES to HDF5: Script completed at {now}")
    print('-----------------------------------------------')
